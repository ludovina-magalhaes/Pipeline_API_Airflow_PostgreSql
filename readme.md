
# Pipeline_API_Airflow_PostgreSql

Este reposit√≥rio cont√©m um pipeline orquestrado no **Airflow** que interage com uma API e persiste dados numa base de dados **PostgreSQL**.
Cria√ß√£o de um pipeline de dados que extrai informa√ß√µes de uma API, transforma os dados com Python e carrega em um banco de dados
Para isso neste projeto vou usar Random User Generator, uma API gratuita e de c√≥digo aberto para gerar dados aleat√≥rios de usu√°rios. Este tipo de API √© √≥tima para treinar e ver a vulnerabilidade e usabilidade dos dados sem expor dados reais.
A ideia surgiu da necessidade de estruturar dados provenientes de uma API de forma organizada e confi√°vel, garantindo consist√™ncia e integridade para futuras an√°lises.
Arquitetura: 
<img width="1605" height="688" alt="image" src="https://github.com/user-attachments/assets/d9c3f0d3-fbf9-4e38-86a8-8d4559a0fc34" />

---

## üìÇ Estrutura do Projeto

| Pasta / Ficheiro | Descri√ß√£o |
|------------------|-----------|
| `dags/` | DAGs do Airflow com tasks definidas para chamar a API e processar dados |
| `plugins/` | Plugins personalizados do Airflow (se existirem) |
| `docker-compose.yaml` | Defini√ß√£o dos servi√ßos Docker: Airflow, PostgreSQL, webserver, scheduler etc. |
| `dockerfile` | Dockerfile para construir a imagem personalizada, se aplic√°vel |
| `requirements.txt` | Depend√™ncias Python necess√°rias para o pipeline |
| `logs/` | Diret√≥rio de logs do Airflow (n√£o versionado) |
| `.gitignore` | Arquivos e pastas ignorados no Git |
| `readme.md` | Este ficheiro de documenta√ß√£o |
| `.env` | (n√£o commitado) Vari√°veis de ambiente sens√≠veis |

---

## üõ†Ô∏è Requisitos

- Docker & Docker Compose  
- Python 3.x  
- Ter configurado vari√°vel(s) de ambiente (ex: credenciais API, credenciais BD) via `.env`  
- Base de dados PostgreSQL (local ou no Docker)  

---
O pipeline foi composto por:
‚Ä¢	Coleta autom√°tica de dados: utiliza√ß√£o da API Random User para obter informa√ß√£o simulada de utilizadores.
‚Ä¢	Armazenamento inicial em base de dados relacional: ingest√£o dos dados brutos na base PostgreSQL, especificamente na inst√¢ncia criada para o projeto (api_random_user).
‚Ä¢	Normaliza√ß√£o e modelagem relacional: cria√ß√£o de tabelas normalizadas diretamente em SQL, seguindo uma abordagem de Data Warehouse, com dimens√µes (dim_user, dim_address, dim_login, dim_id, dim_picture) e uma tabela fato (fact_user). Este processo garantiu consist√™ncia, elimina√ß√£o de duplicados e integridade referencial.
‚Ä¢	Execu√ß√£o e valida√ß√£o em ambiente de administra√ß√£o: utiliza√ß√£o do pgAdmin para implementar queries de transforma√ß√£o, tratar tipos de dados (date, numeric, timestamp) e validar a qualidade da carga.
Este projeto ilustra um cen√°rio pr√°tico de ETL com foco em Data Engineering, no qual dados de origem semi-estruturada s√£o organizados em modelo relacional, servindo como base s√≥lida para an√°lises futuras e constru√ß√£o de dashboards.

O ficheiro requirements.txt √© fundamental em qualquer projeto Python. Ele define todas as bibliotecas necess√°rias para que o projeto funcione corretamente. Com ele, qualquer pessoa pode preparar o ambiente de forma r√°pida e consistente. Este requirements.txt mostra que o projeto combina orquestra√ß√£o de dados (Airflow), armazenamento e consultas em PostgreSQL e modelos de intelig√™ncia artificial da Hugging Face.
<img width="1600" height="193" alt="image" src="https://github.com/user-attachments/assets/fffe670a-c9fd-457b-8131-f31827c58d75" />

O Dockerfile √© um ficheiro usado para personalizar o ambiente do Airflow dentro do Docker. Ele √© √∫til quando os teus DAGs precisam de bibliotecas Python extras que n√£o v√™m instaladas na imagem oficial do Airflow. A fun√ß√£o dele √© manter o ambiente isolado dentro do Docker.
Um Dockerfile simples tem apenas 3 instru√ß√µes:
<img width="1618" height="234" alt="image" src="https://github.com/user-attachments/assets/562da6d9-0921-4c95-ad94-1185543c1fab" />

Assim, quando executas docker compose up, o Docker constr√≥i a imagem j√° com todas as depend√™ncias e inicia o Airflow preparado para as DAGs. O Dockerfile √© o passo que transforma o Airflow "padr√£o" no Airflow "√† nossa medida", incluindo todas as bibliotecas extra que os teus projetos precisam.
<img width="1610" height="558" alt="image" src="https://github.com/user-attachments/assets/d283d6b5-b858-41d3-a846-35f15e6d6837" />

O c√≥digo apresentado define um pipeline ETL (Extract, Transform, Load) utilizando o Apache Airflow como orquestrador de tarefas. Ele tem como objetivo consumir dados da API p√∫blica Random User, transform√°-los num formato estruturado e carreg√°-los para uma tabela no PostgreSQL.
O pipeline est√° organizado em tr√™s fases principais:
1.	Extra√ß√£o (Extract)
A primeira tarefa, extract_random_user_data, realiza uma requisi√ß√£o √† API https://randomuser.me/api/, pedindo 100 utilizadores aleat√≥rios. Os dados retornados em formato JSON s√£o enviados para o mecanismo de comunica√ß√£o do Airflow (XCom) para que possam ser utilizados pelas pr√≥ximas etapas.
2.	Transforma√ß√£o (Transform)
A segunda tarefa, transform_random_user_data, l√™ os dados brutos da etapa anterior e reorganiza-os num dicion√°rio padronizado, facilitando a carga na base de dados.
o	S√£o extra√≠dos atributos como nome, email, morada, localiza√ß√£o geogr√°fica, credenciais de login, entre outros.
o	Converte-se o campo postcode para string (garantindo consist√™ncia).
o	√â adicionado um campo adicional chamado etl_timestamp, que regista o momento exato em que a transforma√ß√£o ocorreu, √∫til para auditoria e rastreabilidade.
3.	Carga (Load)
A terceira tarefa, load_random_user_postgres, conecta-se a uma inst√¢ncia PostgreSQL utilizando a biblioteca psycopg2.
o	Se a tabela random_user_data ainda n√£o existir, ela √© criada com todas as colunas necess√°rias.
o	Cada registo √© inserido na base de dados. Durante este processo, os campos de data s√£o convertidos para o tipo TIMESTAMP do PostgreSQL.
o	Ao final, a transa√ß√£o √© confirmada com commit() e a conex√£o √© encerrada.
Estrutura da DAG
A DAG chama-se random_user_pipeline e tem as seguintes configura√ß√µes:
‚Ä¢	Execu√ß√£o di√°ria (@daily).
‚Ä¢	In√≠cio a 1 de janeiro de 2025.
‚Ä¢	Reexecu√ß√£o autom√°tica em caso de falha (1 retry).
‚Ä¢	Ordem de execu√ß√£o:
task1_get_random_user ‚Üí task2_clean_data ‚Üí task3_insert_postgres.
Conclus√£o
Este pipeline exemplifica bem a utiliza√ß√£o do Airflow para orquestrar processos de ETL:
‚Ä¢	A extra√ß√£o automatiza a coleta de dados de uma API p√∫blica.
‚Ä¢	A transforma√ß√£o garante consist√™ncia e qualidade nos dados antes de carreg√°-los.
‚Ä¢	A carga integra os dados num reposit√≥rio relacional (PostgreSQL), possibilitando an√°lises posteriores.

Execu√ß√£o do Docker com os ficheiros todos j√° feitos :
Verificar se o Docker est√° a correr
docker ps
Mostra os containers em execu√ß√£o.
Subir os servi√ßos (iniciar Airflow + Postgres + Redis + etc.)
docker compose up -d
Parar os servi√ßos
docker compose down
Para e remove todos os containers definidos no docker-compose.yaml.
Reconstruir a imagem (se alteraste o Dockerfile ou requirements.txt)
docker compose build
E depois voltas a subir:
docker compose up -d

Resumindo o fluxo t√≠pico para trabalhares com o Airflow no Docker:
1.	docker compose build (se tiveres Dockerfile personalizado).
2.	docker compose up -d (sobe os servi√ßos).
3.	Abrir o Airflow no navegador: http://localhost:8080.
4.	docker compose down (quando quiseres parar).


1. O que √© o Airflow
O Apache Airflow √© uma ferramenta usada para orquestra√ß√£o de workflows, baseada em DAGs (Directed Acyclic Graphs).
‚Ä¢	DAGs: representam fluxos de tarefas que seguem uma ordem espec√≠fica. N√£o h√° ciclos, ou seja, uma tarefa n√£o pode voltar para a anterior.
‚Ä¢	Pipelines ETL: esta estrutura torna o Airflow ideal para processos de Extra√ß√£o, Transforma√ß√£o e Carga (ETL), seguindo a sequ√™ncia Extrair ‚Üí Transformar ‚Üí Carregar.
2. Arquitetura do Airflow com Docker
A forma mais simples de instalar e executar o Airflow √© com Docker e Docker Compose. A arquitetura t√≠pica inclui:
<img width="886" height="361" alt="image" src="https://github.com/user-attachments/assets/14ec6fd0-876f-4dad-9c7a-7314638449a7" />

3. Funcionamento de um DAG no Airflow
Os DAGs s√£o definidos em ficheiros Python.
Exemplo de um pipeline ETL que extrai dados da API e carrega para uma base Postgres:
1.	Extract Task - obt√©m dados (ex: 50 modelos mais recentes), envia os brutos para o XCOM.
2.	Transform Task - recebe dados do XCOM, remove duplicados, trata nulos, normaliza colunas, envia dados limpos de volta ao XCOM.
3.	Load Task - l√™ dados transformados do XCOM e insere no Postgres atrav√©s de um Postgres, com conex√£o configurada no Airflow UI.
‚Ä¢	XCOM: mecanismo de troca de dados entre tarefas.
‚Ä¢	PythonOperator: define tarefas em Python dentro do DAG.
‚Ä¢	Conex√µes (Admin > Connections): configuram credenciais e par√¢metros para bases externas (ex: host do Postgres no Docker, nome da base pg_db).
4. Gest√£o de Depend√™ncias
Para incluir bibliotecas necess√°rias no ambiente Airflow (ex: psycopg2, huggingface):
‚Ä¢	requirements.txt: lista de pacotes Python.
‚Ä¢	Dockerfile: copia o requirements.txt e instala depend√™ncias com pip.
‚Ä¢	docker-compose.yaml: ajustado para usar a build personalizada, garantindo que a imagem do Airflow tenha todas as depend√™ncias.
<img width="886" height="339" alt="image" src="https://github.com/user-attachments/assets/d8f8f246-d272-4163-983a-25a3c9386874" />

PostgreSQL e PG Admin no Pipeline Airflow
O PostgreSQL (Psql/Postgress) e o PG Admin s√£o componentes essenciais para hospedar e gerir a base de dados utilizada pelo Apache Airflow em pipelines ETL.
PostgreSQL (Psql/Postgress)
‚Ä¢	Fun√ß√£o no ETL: Serve como banco de dados backend e destino final (Load) dos dados. Exemplo: extrair dados de modelos de IA, transformar e carregar no Psql.
‚Ä¢	Hospedagem: Alojado no ambiente Docker e gerido pelo Docker Compose.
‚Ä¢	Conex√£o Airflow: √â necess√°rio configurar a conex√£o no Airflow UI (Admin > Connections). O Host deve ser Postgress (com P mai√∫sculo) para que o Docker encontre o servi√ßo automaticamente, mesmo que o IP mude.
‚Ä¢	Base de Dados: Criar uma base espec√≠fica, ex: PG_DB.
‚Ä¢	Credenciais Padr√£o: Username: airflow, 
‚Ä¢	Password: airflow,
‚Ä¢	 Porta: 5432.
PG Admin
‚Ä¢	Fun√ß√£o Principal: Interface para explorar, gerir e consultar os dados no PostgreSQL. Permite verificar a inser√ß√£o correta dos dados, e para confirmar se todos os modelos foram carregados.
‚Ä¢	Hospedagem em Docker: Servi√ßo inclu√≠do manualmente no docker-compose.yaml, normalmente abaixo do Postgres.
‚Ä¢	Configura√ß√£o: Definir credenciais e porta no Docker Compose:
o	Email: admin@admin.com
o	Password: root
o	Porta: 5050
‚Ä¢	Servidor PG Admin: Ap√≥s aceder √† interface (localhost:5050), adicionar um novo servidor:
o	Nome do Host: postgress
o	Credenciais: Username airflow, Password airflow
<img width="489" height="797" alt="image" src="https://github.com/user-attachments/assets/86772e40-aff1-4a1c-8c90-0eafb97de5a8" />


## üîß Como usar

1. Clona este reposit√≥rio:
   ```bash
   git clone https://github.com/ludovina-magalhaes/Pipeline_API_Airflow_PostgreSql.git
   cd Pipeline_API_Airflow_PostgreSql


Criar ficheiro .env com configura√ß√µes necess√°rias, por exemplo:
POSTGRES_USER=usuario
POSTGRES_PASSWORD=senha
POSTGRES_DB=nome_db
API_URL=https://exemplo.com/api


 Testes & Debugging
Ver logs do Airflow para verificar execu√ß√£o das tasks
Confirmar liga√ß√£o √† base de dados PostgreSQL
Se houver falha de API: examinar endpoint, headers e dados retornados

Licen√ßa
Este projeto est√° sob a licen√ßa MIT (ou outra que escolheres) ‚Äî podes modificar para a licen√ßa que preferires.

